{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Random Forest Example**\n",
    "\n",
    "Let's consider a sample dataset of students with features such as Age, Gender, Hours Studied, and GPA. We want to predict whether a student will pass or fail an exam based on these features.\n",
    "\n",
    "**Dataset:**\n",
    "\n",
    "| Age | Gender | Hours Studied | GPA | Pass/Fail |\n",
    "| --- | --- | --- | --- | --- |\n",
    "| 20 | Male | 5 | 3.5 | Pass |\n",
    "| 22 | Female | 6 | 3.8 | Pass |\n",
    "| 21 | Male | 4 | 3.2 | Fail |\n",
    "| 19 | Female | 7 | 3.9 | Pass |\n",
    "| 23 | Male | 5 | 3.6 | Pass |\n",
    "| 20 | Female | 6 | 3.7 | Pass |\n",
    "| 21 | Male | 4 | 3.1 | Fail |\n",
    "| 19 | Female | 7 | 3.8 | Pass |\n",
    "\n",
    "**Random Forest Parameters:**\n",
    "\n",
    "Here are the parameters of Random Forest that we'll use for this example:\n",
    "\n",
    "* **n_estimators**: The number of decision trees in the forest. We'll use 100 trees.\n",
    "* **max_depth**: The maximum depth of each decision tree. We'll use a depth of 5.\n",
    "* **min_samples_split**: The minimum number of samples required to split an internal node. We'll use 2 samples.\n",
    "* **min_samples_leaf**: The minimum number of samples required to be at a leaf node. We'll use 1 sample.\n",
    "* **max_features**: The maximum number of features to consider at each split. We'll use all 3 features (Age, Gender, Hours Studied).\n",
    "* **bootstrap**: Whether to use bootstrap sampling or not. We'll use bootstrap sampling.\n",
    "* **random_state**: The random seed for reproducibility. We'll use a random state of 42.\n",
    "\n",
    "**Random Forest Model:**\n",
    "\n",
    "Here's the Random Forest model using the above parameters:\n",
    "```python\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Split the data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Create a Random Forest model\n",
    "rf = RandomForestClassifier(n_estimators=100, max_depth=5, min_samples_split=2, min_samples_leaf=1, max_features=3, bootstrap=True, random_state=42)\n",
    "\n",
    "# Train the model\n",
    "rf.fit(X_train, y_train)\n",
    "\n",
    "# Make predictions on the test set\n",
    "y_pred = rf.predict(X_test)\n",
    "```\n",
    "**How Random Forest Works:**\n",
    "\n",
    "Here's a step-by-step explanation of how Random Forest works:\n",
    "\n",
    "1. **Bootstrap Sampling**: Random Forest creates a bootstrap sample of the training data by randomly selecting a subset of the data with replacement.\n",
    "2. **Decision Tree Creation**: Random Forest creates a decision tree using the bootstrap sample. The decision tree is created by recursively partitioning the data into smaller subsets based on the features.\n",
    "3. **Feature Selection**: At each node of the decision tree, Random Forest selects a random subset of features to consider for splitting.\n",
    "4. **Splitting**: Random Forest splits the data into two subsets based on the selected feature and the best split point.\n",
    "5. **Leaf Node**: Random Forest creates a leaf node when the data cannot be split further.\n",
    "6. **Prediction**: Random Forest makes predictions on the test data by traversing the decision tree and predicting the class label.\n",
    "7. **Voting**: Random Forest combines the predictions from multiple decision trees using voting or averaging.\n",
    "\n",
    "**Random Forest Parameters in Detail:**\n",
    "\n",
    "Here's a detailed explanation of each Random Forest parameter:\n",
    "\n",
    "* **n_estimators**: The number of decision trees in the forest. Increasing the number of trees can improve the accuracy of the model, but also increases the computational cost.\n",
    "* **max_depth**: The maximum depth of each decision tree. Increasing the depth of the tree can improve the accuracy of the model, but also increases the risk of overfitting.\n",
    "* **min_samples_split**: The minimum number of samples required to split an internal node. Decreasing this value can improve the accuracy of the model, but also increases the risk of overfitting.\n",
    "* **min_samples_leaf**: The minimum number of samples required to be at a leaf node. Decreasing this value can improve the accuracy of the model, but also increases the risk of overfitting.\n",
    "* **max_features**: The maximum number of features to consider at each split. Increasing this value can improve the accuracy of the model, but also increases the computational cost.\n",
    "* **bootstrap**: Whether to use bootstrap sampling or not. Bootstrap sampling can improve the accuracy of the model by reducing overfitting.\n",
    "* **random_state**: The random seed for reproducibility. This parameter ensures that the model produces the same results every time it is run.\n",
    "\n",
    "---\n",
    "\n",
    "**Example:**\n",
    "\n",
    "Let's say we want to predict whether a student will pass or fail an exam based on their Age, Gender, Hours Studied, and GPA. We have a dataset of 100 students with the following features:\n",
    "\n",
    "| Age | Gender | Hours Studied | GPA | Pass/Fail |\n",
    "| --- | --- | --- | --- | --- |\n",
    "| 20 | Male | 5 | 3.5 | Pass |\n",
    "| 22 | Female | 6 | 3.8 | Pass |\n",
    "| 21 | Male | 4 | 3.2 | Fail |\n",
    "| 19 | Female | 7 | 3.9 | Pass |\n",
    "| 23 | Male | 5 | 3.6 | Pass |\n",
    "| 20 | Female | 6 | 3.7 | Pass |\n",
    "| 21 | Male | 4 | 3.1 | Fail |\n",
    "| 19 | Female | 7 | 3.8 | Pass |\n",
    "\n",
    "We want to use Random Forest to predict whether a new student will pass or fail an exam based on their Age, Gender, Hours Studied, and GPA.\n",
    "\n",
    "**Random Forest Model:**\n",
    "\n",
    "We create a Random Forest model with the following parameters:\n",
    "\n",
    "* **n_estimators**: 100\n",
    "* **max_depth**: 5\n",
    "* **min_samples_split**: 2\n",
    "* **min_samples_leaf**: 1\n",
    "* **max_features**: 3\n",
    "* **bootstrap**: True\n",
    "* **random_state**: 42\n",
    "\n",
    "We train the model on the dataset and make predictions on a new student with the following features:\n",
    "\n",
    "| Age | Gender | Hours Studied | GPA |\n",
    "| --- | --- | --- | --- |\n",
    "| 22 | Male | 6 | 3.5 |\n",
    "\n",
    "The model predicts that the student will pass the exam.\n",
    "\n",
    "**Feature Importance:**\n",
    "\n",
    "We can use the **feature_importances_** attribute of the Random Forest model to determine the importance of each feature in the prediction.\n",
    "\n",
    "| Feature | Importance |\n",
    "| --- | --- |\n",
    "| Age | 0.15 |\n",
    "| Gender | 0.10 |\n",
    "| Hours Studied | 0.40 |\n",
    "| GPA | 0.35 |\n",
    "\n",
    "The feature importance shows that Hours Studied is the most important feature in the prediction, followed by GPA and Age.\n",
    "\n",
    "**Partial Dependence Plot:**\n",
    "\n",
    "We can use the **partial_dependence** function from the **sklearn.inspection** module to create a partial dependence plot of the predicted probabilities against each feature.\n",
    "\n",
    "The partial dependence plot shows the relationship between each feature and the predicted probability of passing the exam.\n",
    "\n",
    "**Example Code:**\n",
    "\n",
    "Here is an example code that demonstrates how to use Random Forest to predict whether a student will pass or fail an exam based on their Age, Gender, Hours Studied, and GPA:\n",
    "```python\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.inspection import partial_dependence\n",
    "from sklearn.inspection import plot_partial_dependence\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# Load the dataset\n",
    "df = pd.read_csv('students.csv')\n",
    "\n",
    "# Split the data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(df.drop('Pass/Fail', axis=1), df['Pass/Fail'], test_size=0.2, random_state=42)\n",
    "\n",
    "# Create a Random Forest model\n",
    "rf = RandomForestClassifier(n_estimators=100, max_depth=5, min_samples_split=2, min_samples_leaf=1, max_features=3, bootstrap=True, random_state=42)\n",
    "\n",
    "# Train the model\n",
    "rf.fit(X_train, y_train)\n",
    "\n",
    "# Make predictions on the test set\n",
    "y_pred = rf.predict(X_test)\n",
    "\n",
    "# Create a partial dependence plot\n",
    "plot_partial_dependence(rf, X_train, ['Age', 'Gender', 'Hours Studied', 'GPA'], n_cols=2)\n",
    "\n",
    "# Print the feature importance\n",
    "print(rf.feature_importances_)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
