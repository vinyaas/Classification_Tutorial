{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Evaluation Metrics in Decision Trees**\n",
    "\n",
    "Decision trees are a popular machine learning algorithm used for classification and regression tasks. Evaluating the performance of a decision tree is crucial to understand its accuracy and effectiveness. There are several evaluation metrics used to assess the performance of a decision tree, including:\n",
    "\n",
    "1. **Accuracy**: This is the most common evaluation metric used to measure the performance of a decision tree. It represents the proportion of correctly classified instances out of all instances in the test dataset.\n",
    "2. **Precision**: This metric measures the proportion of true positives (correctly classified instances) out of all positive predictions made by the decision tree.\n",
    "3. **Recall**: This metric measures the proportion of true positives out of all actual positive instances in the test dataset.\n",
    "4. **F1 Score**: This metric is the harmonic mean of precision and recall, providing a balanced measure of both.\n",
    "5. **Mean Squared Error (MSE)**: This metric measures the average squared difference between predicted and actual values in the test dataset.\n",
    "6. **Mean Absolute Error (MAE)**: This metric measures the average absolute difference between predicted and actual values in the test dataset.\n",
    "7. **Root Mean Squared Percentage Error (RMSPE)**: This metric measures the square root of the average squared percentage difference between predicted and actual values in the test dataset.\n",
    "8. **Coefficient of Determination (R-squared)**: This metric measures the proportion of variance in the dependent variable that is predictable from the independent variables.\n",
    "\n",
    "**Interpretation of Evaluation Metrics**\n",
    "\n",
    "To illustrate the interpretation of these evaluation metrics, let's consider an example using sales data.\n",
    "\n",
    "Suppose we have a decision tree model that predicts the likelihood of a customer making a purchase based on their demographic characteristics, such as age, income, and location. We have a test dataset with 1000 instances, where 200 instances are positive (i.e., the customer made a purchase) and 800 instances are negative (i.e., the customer did not make a purchase).\n",
    "\n",
    "The decision tree model predicts 250 instances as positive and 750 instances as negative. Out of the 250 predicted positive instances, 180 are true positives (i.e., the customer actually made a purchase) and 70 are false positives (i.e., the customer did not make a purchase).\n",
    "\n",
    "**Accuracy**\n",
    "\n",
    "Accuracy = (True Positives + True Negatives) / Total Instances\n",
    "= (180 + 680) / 1000\n",
    "= 86%\n",
    "\n",
    "The accuracy of the decision tree model is 86%, indicating that it correctly classified 86% of the instances in the test dataset.\n",
    "\n",
    "**Precision**\n",
    "\n",
    "Precision = True Positives / (True Positives + False Positives)\n",
    "= 180 / (180 + 70)\n",
    "= 72%\n",
    "\n",
    "The precision of the decision tree model is 72%, indicating that out of all positive predictions made by the model, 72% were correct.\n",
    "\n",
    "**Recall**\n",
    "\n",
    "Recall = True Positives / (True Positives + False Negatives)\n",
    "= 180 / (180 + 20)\n",
    "= 90%\n",
    "\n",
    "The recall of the decision tree model is 90%, indicating that out of all actual positive instances in the test dataset, 90% were correctly classified by the model.\n",
    "\n",
    "**F1 Score**\n",
    "\n",
    "F1 Score = 2 \\* (Precision \\* Recall) / (Precision + Recall)\n",
    "= 2 \\* (0.72 \\* 0.90) / (0.72 + 0.90)\n",
    "= 0.81\n",
    "\n",
    "The F1 score of the decision tree model is 0.81, indicating a balanced measure of precision and recall.\n",
    "\n",
    "**Mean Squared Error (MSE)**\n",
    "\n",
    "MSE = (1/n) \\* ∑(Predicted - Actual)^2\n",
    "= (1/1000) \\* ∑(Predicted - Actual)^2\n",
    "= 10.23\n",
    "\n",
    "The MSE of the decision tree model is 10.23, indicating the average squared difference between predicted and actual values in the test dataset.\n",
    "\n",
    "**Mean Absolute Error (MAE)**\n",
    "\n",
    "MAE = (1/n) \\* ∑|Predicted - Actual|\n",
    "= (1/1000) \\* ∑|Predicted - Actual|\n",
    "= 8.15\n",
    "\n",
    "The MAE of the decision tree model is 8.15, indicating the average absolute difference between predicted and actual values in the test dataset.\n",
    "\n",
    "**Root Mean Squared Percentage Error (RMSPE)**\n",
    "\n",
    "RMSPE = √((1/n) \\* ∑((Predicted - Actual) / Actual)^2)\n",
    "= √((1/1000) \\* ∑((Predicted - Actual) / Actual)^2)\n",
    "= 12.56%\n",
    "\n",
    "The RMSPE of the decision tree model is 12.56%, indicating the square root of the average squared percentage difference between predicted and actual values in the test dataset.\n",
    "\n",
    "**Coefficient of Determination (R-squared)**\n",
    "\n",
    "R-squared = 1 - (SSE / SST)\n",
    "= 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "**True Positives, True Negatives, False Positives, and False Negatives**\n",
    "\n",
    "In the context of classification problems, we have four possible outcomes:\n",
    "\n",
    "1. **True Positives (TP)**: These are the instances that are correctly predicted as positive by the model.\n",
    "2. **True Negatives (TN)**: These are the instances that are correctly predicted as negative by the model.\n",
    "3. **False Positives (FP)**: These are the instances that are incorrectly predicted as positive by the model.\n",
    "4. **False Negatives (FN)**: These are the instances that are incorrectly predicted as negative by the model.\n",
    "\n",
    "**Example**\n",
    "\n",
    "Let's consider a simple example to illustrate these concepts. Suppose we have a medical test that can detect whether a person has a disease or not. We have a dataset of 100 people, where 50 people have the disease (positive) and 50 people do not have the disease (negative).\n",
    "\n",
    "The medical test predicts the following results:\n",
    "\n",
    "* 40 people with the disease are correctly predicted as positive (TP)\n",
    "* 10 people with the disease are incorrectly predicted as negative (FN)\n",
    "* 5 people without the disease are incorrectly predicted as positive (FP)\n",
    "* 45 people without the disease are correctly predicted as negative (TN)\n",
    "\n",
    "**True Positives (TP)**\n",
    "\n",
    "* 40 people with the disease are correctly predicted as positive\n",
    "* Example: John has the disease and the test predicts that he has the disease. This is a true positive.\n",
    "\n",
    "**True Negatives (TN)**\n",
    "\n",
    "* 45 people without the disease are correctly predicted as negative\n",
    "* Example: Emily does not have the disease and the test predicts that she does not have the disease. This is a true negative.\n",
    "\n",
    "**False Positives (FP)**\n",
    "\n",
    "* 5 people without the disease are incorrectly predicted as positive\n",
    "* Example: Michael does not have the disease, but the test predicts that he has the disease. This is a false positive.\n",
    "\n",
    "**False Negatives (FN)**\n",
    "\n",
    "* 10 people with the disease are incorrectly predicted as negative\n",
    "* Example: Sarah has the disease, but the test predicts that she does not have the disease. This is a false negative.\n",
    "\n",
    "**Summary**\n",
    "\n",
    "|  | Predicted Positive | Predicted Negative |\n",
    "| --- | --- | --- |\n",
    "| **Actual Positive** | 40 (TP) | 10 (FN) |\n",
    "| **Actual Negative** | 5 (FP) | 45 (TN) |\n",
    "\n",
    "In this example, the true positives are the 40 people with the disease who are correctly predicted as positive. The true negatives are the 45 people without the disease who are correctly predicted as negative. The false positives are the 5 people without the disease who are incorrectly predicted as positive, and the false negatives are the 10 people with the disease who are incorrectly predicted as negative.\n",
    "\n",
    "**Metrics**\n",
    "\n",
    "We can calculate various metrics using these values, such as:\n",
    "\n",
    "* **Accuracy**: (TP + TN) / (TP + TN + FP + FN) = (40 + 45) / (40 + 45 + 5 + 10) = 85%\n",
    "* **Precision**: TP / (TP + FP) = 40 / (40 + 5) = 88.9%\n",
    "* **Recall**: TP / (TP + FN) = 40 / (40 + 10) = 80%\n",
    "* **F1 Score**: 2 \\* (Precision \\* Recall) / (Precision + Recall) = 2 \\* (0.889 \\* 0.8) / (0.889 + 0.8) = 0.844\n",
    "\n",
    "These metrics can help us evaluate the performance of the medical test and identify areas for improvement."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "**Interpreting Metrics: What Range of Values Indicates a Very Good Model?**\n",
    "\n",
    "When evaluating the performance of a model, it's essential to understand the range of values for each metric that indicate a very good model. Here are some general guidelines:\n",
    "\n",
    "1. **Accuracy**:\n",
    "\t* 90-100%: Excellent (very good model)\n",
    "\t* 80-89%: Good (model is performing well)\n",
    "\t* 70-79%: Fair (model needs improvement)\n",
    "\t* Below 70%: Poor (model is not performing well)\n",
    "2. **Precision**:\n",
    "\t* 0.9-1.0: Excellent (very good model)\n",
    "\t* 0.8-0.89: Good (model is performing well)\n",
    "\t* 0.7-0.79: Fair (model needs improvement)\n",
    "\t* Below 0.7: Poor (model is not performing well)\n",
    "3. **Recall**:\n",
    "\t* 0.9-1.0: Excellent (very good model)\n",
    "\t* 0.8-0.89: Good (model is performing well)\n",
    "\t* 0.7-0.79: Fair (model needs improvement)\n",
    "\t* Below 0.7: Poor (model is not performing well)\n",
    "4. **F1 Score**:\n",
    "\t* 0.9-1.0: Excellent (very good model)\n",
    "\t* 0.8-0.89: Good (model is performing well)\n",
    "\t* 0.7-0.79: Fair (model needs improvement)\n",
    "\t* Below 0.7: Poor (model is not performing well)\n",
    "5. **Mean Squared Error (MSE)**:\n",
    "\t* Close to 0: Excellent (very good model)\n",
    "\t* 0.1-1.0: Good (model is performing well)\n",
    "\t* 1.0-10.0: Fair (model needs improvement)\n",
    "\t* Above 10.0: Poor (model is not performing well)\n",
    "6. **Mean Absolute Error (MAE)**:\n",
    "\t* Close to 0: Excellent (very good model)\n",
    "\t* 0.1-1.0: Good (model is performing well)\n",
    "\t* 1.0-10.0: Fair (model needs improvement)\n",
    "\t* Above 10.0: Poor (model is not performing well)\n",
    "7. **Root Mean Squared Percentage Error (RMSPE)**:\n",
    "\t* Close to 0: Excellent (very good model)\n",
    "\t* 0.1-10.0: Good (model is performing well)\n",
    "\t* 10.0-50.0: Fair (model needs improvement)\n",
    "\t* Above 50.0: Poor (model is not performing well)\n",
    "8. **Coefficient of Determination (R-squared)**:\n",
    "\t* 0.9-1.0: Excellent (very good model)\n",
    "\t* 0.8-0.89: Good (model is performing well)\n",
    "\t* 0.7-0.79: Fair (model needs improvement)\n",
    "\t* Below 0.7: Poor (model is not performing well)\n",
    "\n",
    "Keep in mind that these are general guidelines, and the specific range of values that indicate a very good model may vary depending on the problem, dataset, and industry.\n",
    "\n",
    "**Example**\n",
    "\n",
    "Suppose we have a model that predicts house prices, and we evaluate its performance using the metrics above. If the model has:\n",
    "\n",
    "* Accuracy: 95%\n",
    "* Precision: 0.92\n",
    "* Recall: 0.90\n",
    "* F1 Score: 0.91\n",
    "* MSE: 0.05\n",
    "* MAE: 0.10\n",
    "* RMSPE: 5.0%\n",
    "* R-squared: 0.95\n",
    "\n",
    "Based on the guidelines above, we can conclude that this model is very good, as it has high accuracy, precision, recall, and F1 score, and low MSE, MAE, and RMSPE. The R-squared value also indicates that the model is explaining a large proportion of the variance in the data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "While recall and precision are related, they are not the same thing.\n",
    "\n",
    "**Recall** measures the proportion of actual positive instances (customers who made a purchase) that were correctly predicted by the model. It answers the question: \"Out of all the customers who actually made a purchase, how many did the model correctly predict?\"\n",
    "\n",
    "**Precision** measures the proportion of positive predictions (customers predicted to make a purchase) that were correct. It answers the question: \"Out of all the customers the model predicted would make a purchase, how many actually did?\"\n",
    "\n",
    "The key difference between recall and precision is the direction of the comparison:\n",
    "\n",
    "* Recall compares the model's predictions to the actual positive instances (customers who made a purchase).\n",
    "* Precision compares the model's predictions to the actual outcomes (customers who actually made a purchase).\n",
    "\n",
    "To illustrate the difference, let's use an example:\n",
    "\n",
    "Suppose the model predicts that 250 customers will make a purchase, but only 180 of them actually do. In this case:\n",
    "\n",
    "* Recall (90%): The model correctly predicted 180 out of 200 actual customers who made a purchase (200 is the total number of customers who made a purchase).\n",
    "* Precision (72%): The model correctly predicted 180 out of 250 customers who were predicted to make a purchase (250 is the total number of customers predicted to make a purchase).\n",
    "\n",
    "As you can see, recall and precision are related but distinct metrics. Recall focuses on the model's ability to detect actual positive instances, while precision focuses on the model's ability to make accurate predictions.\n",
    "\n",
    "To help you remember the difference:\n",
    "\n",
    "* Recall: \"How many actual positives did the model catch?\"\n",
    "* Precision: \"How many of the model's positive predictions were correct?\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Classification Report: A Detailed Explanation**\n",
    "\n",
    "A classification report is a summary of the performance of a classification model, including metrics such as precision, recall, F1 score, and support (the number of true instances in each class). The report is typically generated using the `classification_report` function from the `sklearn.metrics` module in Python.\n",
    "\n",
    "**Components of a Classification Report:**\n",
    "\n",
    "1. **Precision**: The ratio of true positives to the sum of true positives and false positives.\n",
    "2. **Recall**: The ratio of true positives to the sum of true positives and false negatives.\n",
    "3. **F1 Score**: The harmonic mean of precision and recall.\n",
    "4. **Support**: The number of true instances in each class.\n",
    "\n",
    "**Interpreting a Classification Report:**\n",
    "\n",
    "Let's use the same customer purchase example to illustrate how to interpret a classification report.\n",
    "\n",
    "Suppose we have a classification model that predicts whether a customer will make a purchase or not. The model is trained on a dataset with 1000 instances, where 200 instances are positive (i.e., the customer made a purchase) and 800 instances are negative (i.e., the customer did not make a purchase).\n",
    "\n",
    "The classification report for this model might look like this:\n",
    "```\n",
    "              precision    recall  f1-score   support\n",
    "\n",
    "       0       0.95      0.98      0.96      800\n",
    "       1       0.80      0.75      0.77      200\n",
    "\n",
    "    accuracy                           0.92      1000\n",
    "   macro avg       0.88      0.86      0.86      1000\n",
    "weighted avg       0.93      0.92      0.92      1000\n",
    "```\n",
    "**Understanding the Report:**\n",
    "\n",
    "1. **Class 0 (Negative)**: The precision is 0.95, which means that out of all the instances predicted as negative, 95% were actually negative. The recall is 0.98, which means that out of all the actual negative instances, 98% were correctly predicted as negative. The F1 score is 0.96, which is the harmonic mean of precision and recall. The support is 800, which is the number of true negative instances.\n",
    "2. **Class 1 (Positive)**: The precision is 0.80, which means that out of all the instances predicted as positive, 80% were actually positive. The recall is 0.75, which means that out of all the actual positive instances, 75% were correctly predicted as positive. The F1 score is 0.77, which is the harmonic mean of precision and recall. The support is 200, which is the number of true positive instances.\n",
    "3. **Accuracy**: The overall accuracy of the model is 0.92, which means that out of all the instances, 92% were correctly predicted.\n",
    "4. **Macro Average**: The macro average precision, recall, and F1 score are calculated by taking the average of the metrics for each class. In this case, the macro average precision is 0.88, recall is 0.86, and F1 score is 0.86.\n",
    "5. **Weighted Average**: The weighted average precision, recall, and F1 score are calculated by taking the average of the metrics for each class, weighted by the support (number of true instances) for each class. In this case, the weighted average precision is 0.93, recall is 0.92, and F1 score is 0.92.\n",
    "\n",
    "**Insights from the Report:**\n",
    "\n",
    "1. The model is more accurate for negative instances (class 0) than for positive instances (class 1).\n",
    "2. The model has a higher recall for negative instances (0.98) than for positive instances (0.75), which means that it is more likely to correctly predict a negative instance than a positive instance.\n",
    "3. The model has a higher precision for negative instances (0.95) than for positive instances (0.80), which means that it is more confident in its predictions for negative instances than for positive instances.\n",
    "4. The overall accuracy of the model is high (0.92), but the macro average and weighted average metrics suggest that the model may be biased towards the majority class (negative instances)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "**Confusion Matrix: A Detailed Explanation**\n",
    "\n",
    "A confusion matrix is a table that summarizes the predictions against the actual outcomes. It provides a detailed view of the model's performance, including the number of true positives, true negatives, false positives, and false negatives.\n",
    "\n",
    "**Components of a Confusion Matrix:**\n",
    "\n",
    "1. **True Positives (TP)**: The number of instances that are correctly predicted as positive.\n",
    "2. **True Negatives (TN)**: The number of instances that are correctly predicted as negative.\n",
    "3. **False Positives (FP)**: The number of instances that are incorrectly predicted as positive.\n",
    "4. **False Negatives (FN)**: The number of instances that are incorrectly predicted as negative.\n",
    "\n",
    "**Interpreting a Confusion Matrix:**\n",
    "\n",
    "Let's use the same customer purchase example to illustrate how to interpret a confusion matrix.\n",
    "\n",
    "Suppose we have a classification model that predicts whether a customer will make a purchase or not. The model is trained on a dataset with 1000 instances, where 200 instances are positive (i.e., the customer made a purchase) and 800 instances are negative (i.e., the customer did not make a purchase).\n",
    "\n",
    "The confusion matrix for this model might look like this:\n",
    "```\n",
    "              Predicted Negative  Predicted Positive\n",
    "Actual Negative          760               40\n",
    "Actual Positive           20              180\n",
    "```\n",
    "**Understanding the Matrix:**\n",
    "\n",
    "1. **True Negatives (TN)**: 760 instances were correctly predicted as negative.\n",
    "2. **True Positives (TP)**: 180 instances were correctly predicted as positive.\n",
    "3. **False Positives (FP)**: 40 instances were incorrectly predicted as positive (i.e., the customer did not make a purchase but was predicted to make a purchase).\n",
    "4. **False Negatives (FN)**: 20 instances were incorrectly predicted as negative (i.e., the customer made a purchase but was predicted not to make a purchase).\n",
    "\n",
    "**Metrics Derived from the Confusion Matrix:**\n",
    "\n",
    "1. **Accuracy**: (TP + TN) / (TP + TN + FP + FN) = (760 + 180) / (760 + 180 + 40 + 20) = 0.92\n",
    "2. **Precision**: TP / (TP + FP) = 180 / (180 + 40) = 0.82\n",
    "3. **Recall**: TP / (TP + FN) = 180 / (180 + 20) = 0.90\n",
    "4. **F1 Score**: 2 \\* (Precision \\* Recall) / (Precision + Recall) = 2 \\* (0.82 \\* 0.90) / (0.82 + 0.90) = 0.86\n",
    "\n",
    "**Insights from the Confusion Matrix:**\n",
    "\n",
    "1. The model is more accurate for negative instances (760 true negatives) than for positive instances (180 true positives).\n",
    "2. The model has a higher recall for positive instances (0.90) than for negative instances (0.95), which means that it is more likely to correctly predict a positive instance than a negative instance.\n",
    "3. The model has a higher precision for positive instances (0.82) than for negative instances (0.95), which means that it is more confident in its predictions for positive instances than for negative instances.\n",
    "4. The model has a relatively low number of false positives (40) and false negatives (20), which indicates that it is making few mistakes in its predictions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
