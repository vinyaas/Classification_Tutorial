{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Interpretability Techniques for Decision Trees and Random Forests**\n",
    "\n",
    "Interpretability techniques are methods used to understand and explain the predictions made by machine learning models, including Decision Trees and Random Forests. These techniques can help identify the most important features contributing to the predictions, understand the relationships between the features and the target variable, and provide insights into the model's decision-making process.\n",
    "\n",
    "Here, we'll explore three popular interpretability techniques:\n",
    "\n",
    "1. **SHAP (SHapley Additive exPlanations) values**\n",
    "2. **LIME (Local Interpretable Model-agnostic Explanations)**\n",
    "3. **TreeExplainer**\n",
    "\n",
    "We'll use a sample sales data to demonstrate each technique.\n",
    "\n",
    "**Sample Sales Data**\n",
    "\n",
    "Let's consider a sample sales data with the following features:\n",
    "\n",
    "* `Product`: The type of product sold (e.g., A, B, C)\n",
    "* `Price`: The price of the product\n",
    "* `Advertising`: The amount spent on advertising\n",
    "* `Sales`: The total sales amount\n",
    "\n",
    "Our goal is to predict the `Sales` amount based on the other features.\n",
    "\n",
    "**SHAP Values**\n",
    "\n",
    "SHAP values are a technique used to assign a value to each feature for a specific prediction, indicating its contribution to the outcome. The values are calculated based on the Shapley value concept from game theory, which distributes the total value among players (features) based on their marginal contributions.\n",
    "\n",
    "In our sample sales data, let's say we want to predict the sales amount for a specific product with the following features:\n",
    "\n",
    "* `Product`: A\n",
    "* `Price`: 10\n",
    "* `Advertising`: 100\n",
    "\n",
    "The SHAP values for this prediction might look like this:\n",
    "\n",
    "| Feature | SHAP Value |\n",
    "| --- | --- |\n",
    "| Product | 0.2 |\n",
    "| Price | -0.1 |\n",
    "| Advertising | 0.5 |\n",
    "\n",
    "These values indicate that:\n",
    "\n",
    "* The `Product` feature contributes 0.2 to the predicted sales amount (i.e., the sales amount would be 0.2 lower if the product were not A).\n",
    "* The `Price` feature contributes -0.1 to the predicted sales amount (i.e., the sales amount would be 0.1 higher if the price were not 10).\n",
    "* The `Advertising` feature contributes 0.5 to the predicted sales amount (i.e., the sales amount would be 0.5 lower if the advertising amount were not 100).\n",
    "\n",
    "**LIME**\n",
    "\n",
    "LIME is a technique used to generate an interpretable model locally around a specific prediction. It creates a simplified model that approximates the original model's behavior for a specific instance.\n",
    "\n",
    "In our sample sales data, let's say we want to predict the sales amount for a specific product with the following features:\n",
    "\n",
    "* `Product`: A\n",
    "* `Price`: 10\n",
    "* `Advertising`: 100\n",
    "\n",
    "The LIME model might look like this:\n",
    "\n",
    "`Sales = 0.5 * Advertising + 0.2 * Product - 0.1 * Price`\n",
    "\n",
    "This model indicates that:\n",
    "\n",
    "* The `Advertising` feature has the most significant impact on the sales amount (0.5).\n",
    "* The `Product` feature has a moderate impact on the sales amount (0.2).\n",
    "* The `Price` feature has a negative impact on the sales amount (-0.1).\n",
    "\n",
    "**TreeExplainer**\n",
    "\n",
    "TreeExplainer is a technique used to explain the predictions made by tree-based models, such as Decision Trees and Random Forests. It provides a detailed breakdown of the decision-making process for a specific prediction.\n",
    "\n",
    "In our sample sales data, let's say we want to predict the sales amount for a specific product with the following features:\n",
    "\n",
    "* `Product`: A\n",
    "* `Price`: 10\n",
    "* `Advertising`: 100\n",
    "\n",
    "The TreeExplainer output might look like this:\n",
    "\n",
    "| Node | Feature | Threshold | Prediction |\n",
    "| --- | --- | --- | --- |\n",
    "| 1 | Advertising | 50 | 200 |\n",
    "| 2 | Price | 15 | 150 |\n",
    "| 3 | Product | A | 250 |\n",
    "\n",
    "This output indicates that:\n",
    "\n",
    "* The first node splits on the `Advertising` feature with a threshold of 50. If the advertising amount is greater than 50, the prediction is 200.\n",
    "* The second node splits on the `Price` feature with a threshold of 15. If the price is less than 15, the prediction is 150.\n",
    "* The third node splits on the `Product` feature with a value of A. If the product is A, the prediction is 250.\n",
    "\n",
    "These interpretability techniques provide valuable insights into the decision-making process of the model, helping to understand which features contribute the most to the predictions and how they interact with each other.\n",
    "\n",
    "Here is a sample Python code using the SHAP library to calculate SHAP values for a Random Forest model:\n",
    "```python\n",
    "import pandas as pd\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.model_selection import train_test_split\n",
    "import shap\n",
    "\n",
    "# Load the data\n",
    "data = pd.read_csv('sales_data.csv')\n",
    "\n",
    "# Split the data into features and target\n",
    "X = data.drop('Sales', axis=1)\n",
    "y = data['Sales']\n",
    "\n",
    "# Split the data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Train a Random Forest model\n",
    "rf = RandomForestRegressor(n_estimators=100, random_state=42)\n",
    "rf.fit(X_train, y_train)\n",
    "\n",
    "# Calculate SHAP values\n",
    "explainer = shap.TreeExplainer(rf)\n",
    "shap_values = explainer.shap_values(X_test)\n",
    "\n",
    "# Plot the SHAP values\n",
    "shap.summary_plot(shap_values, X_test, plot_type='bar')\n",
    "\n",
    "# Plot the SHAP values for a specific instance\n",
    "shap.force_plot(explainer.expected_value, shap_values[0,:], X_test.iloc[0,:], matplotlib=True)\n",
    "```\n",
    "This code calculates the SHAP values for a Random Forest model trained on the sales data and plots the SHAP values for a specific instance.\n",
    "\n",
    "**LIME**\n",
    "\n",
    "To use LIME, you can use the following Python code:\n",
    "```python\n",
    "import lime\n",
    "from lime.lime_tabular import LimeTabularExplainer\n",
    "\n",
    "# Create a LIME explainer\n",
    "explainer = LimeTabularExplainer(X_train, feature_names=X_train.columns, class_names=['Sales'], discretize_continuous=True)\n",
    "\n",
    "# Explain a specific instance\n",
    "exp = explainer.explain_instance(X_test.iloc[0], rf.predict, num_features=10)\n",
    "\n",
    "# Plot the LIME explanation\n",
    "exp.as_pyplot_figure()\n",
    "```\n",
    "This code creates a LIME explainer and uses it to explain a specific instance of the sales data.\n",
    "\n",
    "**TreeExplainer**\n",
    "\n",
    "To use TreeExplainer, you can use the following Python code:\n",
    "```python\n",
    "import treeexplainer\n",
    "\n",
    "# Create a TreeExplainer\n",
    "explainer = treeexplainer.TreeExplainer(rf)\n",
    "\n",
    "# Explain a specific instance\n",
    "exp = explainer.explain_instance(X_test.iloc[0])\n",
    "\n",
    "# Plot the TreeExplainer explanation\n",
    "exp.plot()\n",
    "```\n",
    "This code creates a TreeExplainer and uses it to explain a specific instance of the sales data.\n",
    "\n",
    "These are just a few examples of how you can use interpretability techniques to understand the predictions made by a machine learning model. The specific technique you use will depend on the type of model you are using and the type of data you are working with."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "----"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
