{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**What is Class Imbalance?**\n",
    "\n",
    "Class imbalance, also known as class imbalance problem or class skew, refers to a situation where the number of instances in one class is significantly larger than the number of instances in the other classes. In other words, one class has a much larger number of instances than the other classes. This can lead to biased models that are more accurate for the majority class and less accurate for the minority class.\n",
    "\n",
    "**When to Check for Class Imbalance?**\n",
    "\n",
    "Class imbalance should be checked at the beginning of the data analysis process, before building any models. It's essential to identify class imbalance early on to ensure that the models are not biased towards the majority class.\n",
    "\n",
    "**How to Check for Class Imbalance?**\n",
    "\n",
    "To check for class imbalance, you can use the following methods:\n",
    "\n",
    "1. **Class Distribution Plot**: Plot a bar chart or histogram to visualize the class distribution. This will help you to quickly identify if one class has a significantly larger number of instances than the other classes.\n",
    "2. **Class Ratio**: Calculate the ratio of the majority class to the minority class. If the ratio is greater than 1:10, it's likely that you have a class imbalance problem.\n",
    "3. **Class Frequency**: Calculate the frequency of each class. If one class has a frequency that is significantly higher than the other classes, it may indicate class imbalance.\n",
    "\n",
    "**Example:**\n",
    "\n",
    "Let's consider a dataset of customers who have either purchased a product (class 1) or not purchased a product (class 0). The dataset has 1000 instances, with 900 instances in class 0 (not purchased) and 100 instances in class 1 (purchased).\n",
    "\n",
    "| Class | Frequency |\n",
    "| --- | --- |\n",
    "| 0 (Not Purchased) | 900 |\n",
    "| 1 (Purchased) | 100 |\n",
    "\n",
    "In this example, the class ratio is 9:1, indicating that class 0 has a significantly larger number of instances than class 1. This is a classic example of class imbalance.\n",
    "\n",
    "**How Does Class Imbalance Affect Model Performance?**\n",
    "\n",
    "Class imbalance can significantly affect model performance in several ways:\n",
    "\n",
    "1. **Biased Models**: Models may become biased towards the majority class, resulting in poor performance on the minority class.\n",
    "2. **Poor Accuracy**: Models may have poor accuracy on the minority class, leading to incorrect predictions.\n",
    "3. **Overfitting**: Models may overfit the majority class, resulting in poor performance on unseen data.\n",
    "4. **Underfitting**: Models may underfit the minority class, resulting in poor performance on unseen data.\n",
    "\n",
    "**How to Fix Class Imbalance?**\n",
    "\n",
    "There are several techniques to fix class imbalance:\n",
    "\n",
    "1. **Oversampling the Minority Class**: Create additional instances of the minority class by duplicating existing instances or generating new instances using techniques such as SMOTE (Synthetic Minority Over-sampling Technique).\n",
    "2. **Undersampling the Majority Class**: Reduce the number of instances in the majority class by randomly removing instances or using techniques such as Tomek links.\n",
    "3. **Class Weighting**: Assign different weights to each class, with the minority class having a higher weight than the majority class.\n",
    "4. **Anomaly Detection**: Use anomaly detection techniques to identify instances that are likely to be misclassified.\n",
    "5. **Ensemble Methods**: Use ensemble methods such as bagging or boosting to combine multiple models and improve performance on the minority class.\n",
    "\n",
    "**Techniques Widely Used by Data Scientists:**\n",
    "\n",
    "Some of the techniques widely used by data scientists to handle class imbalance include:\n",
    "\n",
    "1. **SMOTE (Synthetic Minority Over-sampling Technique)**: A technique that generates new instances of the minority class by interpolating between existing instances.\n",
    "2. **Tomek Links**: A technique that removes instances from the majority class that are closest to the minority class.\n",
    "3. **Random Oversampling**: A technique that duplicates existing instances of the minority class to increase its size.\n",
    "4. **Random Undersampling**: A technique that randomly removes instances from the majority class to reduce its size.\n",
    "5. **Class Weighting**: A technique that assigns different weights to each class, with the minority class having a higher weight than the majority class.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Example Code:**\n",
    "\n",
    "Here's an example code in Python using the `imbalanced-learn` library to handle class imbalance:\n",
    "```python\n",
    "from imblearn.over_sampling import SMOTE\n",
    "from sklearn.datasets import make_classification\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "\n",
    "# Generate a classification dataset with class imbalance\n",
    "X, y = make_classification(n_samples=1000, n_features=20, n_informative=10, n_redundant=0, n_repeated=0, n_classes=2, n_clusters_per_class=1, weights=[0.9, 0.1], random_state=42)\n",
    "\n",
    "# Split the dataset into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Create a SMOTE object to oversample the minority class\n",
    "smote = SMOTE(random_state=42)\n",
    "\n",
    "# Fit the SMOTE object to the training data and transform the data\n",
    "X_train_res, y_train_res = smote.fit_resample(X_train, y_train)\n",
    "\n",
    "# Train a random forest classifier on the resampled data\n",
    "rf = RandomForestClassifier(n_estimators=100, random_state=42)\n",
    "rf.fit(X_train_res, y_train_res)\n",
    "\n",
    "# Evaluate the model on the test data\n",
    "y_pred = rf.predict(X_test)\n",
    "print(\"Accuracy:\", accuracy_score(y_test, y_pred))\n",
    "print(\"Classification Report:\")\n",
    "print(classification_report(y_test, y_pred))\n",
    "```\n",
    "This code generates a classification dataset with class imbalance, splits the data into training and testing sets, creates a SMOTE object to oversample the minority class, fits the SMOTE object to the training data and transforms the data, trains a random forest classifier on the resampled data, and evaluates the model on the test data.\n",
    "\n",
    "**Real-World Example:**\n",
    "\n",
    "Let's consider a real-world example of class imbalance in the context of credit card fraud detection. Suppose we have a dataset of credit card transactions, where the majority of transactions are legitimate (class 0) and a small minority of transactions are fraudulent (class 1). The dataset has 100,000 transactions, with 99,000 legitimate transactions and 1,000 fraudulent transactions.\n",
    "\n",
    "| Class | Frequency |\n",
    "| --- | --- |\n",
    "| 0 (Legitimate) | 99,000 |\n",
    "| 1 (Fraudulent) | 1,000 |\n",
    "\n",
    "In this example, the class ratio is 99:1, indicating that the legitimate class has a significantly larger number of instances than the fraudulent class. This is a classic example of class imbalance.\n",
    "\n",
    "To handle this class imbalance, we can use techniques such as SMOTE to oversample the minority class (fraudulent transactions) or undersample the majority class (legitimate transactions). We can also use class weighting to assign different weights to each class, with the minority class having a higher weight than the majority class.\n",
    "\n",
    "By handling class imbalance, we can improve the performance of our credit card fraud detection model and reduce the number of false negatives (fraudulent transactions that are misclassified as legitimate).\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**What is SMOTEEN?**\n",
    "\n",
    "SMOTEEN (Synthetic Minority Over-sampling Technique with Ensemble) is a variant of the SMOTE algorithm that combines the benefits of SMOTE with ensemble learning. SMOTEEN is designed to handle class imbalance problems in datasets by oversampling the minority class and creating synthetic samples that are similar to the existing minority class samples.\n",
    "\n",
    "**How does SMOTEEN work?**\n",
    "\n",
    "SMOTEEN works as follows:\n",
    "\n",
    "1. **Identify the minority class**: SMOTEEN identifies the minority class in the dataset, which is the class with the fewest number of instances.\n",
    "2. **Create synthetic samples**: SMOTEEN creates synthetic samples of the minority class by interpolating between existing minority class samples. This is done by calculating the difference between the feature values of the existing minority class samples and adding a random percentage of this difference to the feature values of the existing minority class samples.\n",
    "3. **Create an ensemble**: SMOTEEN creates an ensemble of multiple SMOTE models, each trained on a different subset of the dataset.\n",
    "4. **Combine the models**: SMOTEEN combines the predictions of the individual SMOTE models to create a final prediction.\n",
    "\n",
    "**What does SMOTEEN do?**\n",
    "\n",
    "SMOTEEN does the following:\n",
    "\n",
    "1. **Oversamples the minority class**: SMOTEEN oversamples the minority class by creating synthetic samples that are similar to the existing minority class samples.\n",
    "2. **Reduces overfitting**: SMOTEEN reduces overfitting by creating an ensemble of multiple SMOTE models, each trained on a different subset of the dataset.\n",
    "3. **Improves classification performance**: SMOTEEN improves classification performance by combining the predictions of the individual SMOTE models to create a final prediction.\n",
    "\n",
    "**How does SMOTEEN fix imbalance in class?**\n",
    "\n",
    "SMOTEEN fixes imbalance in class by:\n",
    "\n",
    "1. **Increasing the size of the minority class**: SMOTEEN increases the size of the minority class by creating synthetic samples that are similar to the existing minority class samples.\n",
    "2. **Reducing the impact of the majority class**: SMOTEEN reduces the impact of the majority class by creating an ensemble of multiple SMOTE models, each trained on a different subset of the dataset.\n",
    "3. **Improving the classification performance**: SMOTEEN improves the classification performance by combining the predictions of the individual SMOTE models to create a final prediction.\n",
    "\n",
    "**Advantages of SMOTEEN**\n",
    "\n",
    "SMOTEEN has several advantages, including:\n",
    "\n",
    "1. **Improved classification performance**: SMOTEEN improves classification performance by combining the predictions of the individual SMOTE models to create a final prediction.\n",
    "2. **Reduced overfitting**: SMOTEEN reduces overfitting by creating an ensemble of multiple SMOTE models, each trained on a different subset of the dataset.\n",
    "3. **Increased robustness**: SMOTEEN increases robustness by creating synthetic samples that are similar to the existing minority class samples.\n",
    "\n",
    "**Disadvantages of SMOTEEN**\n",
    "\n",
    "SMOTEEN has several disadvantages, including:\n",
    "\n",
    "1. **Increased computational complexity**: SMOTEEN increases computational complexity by creating an ensemble of multiple SMOTE models, each trained on a different subset of the dataset.\n",
    "2. **Requires careful tuning**: SMOTEEN requires careful tuning of the hyperparameters to achieve optimal performance.\n",
    "3. **May not work well with high-dimensional data**: SMOTEEN may not work well with high-dimensional data, as the number of features can make it difficult to create synthetic samples that are similar to the existing minority class samples.\n",
    "\n",
    "---\n",
    "**Example Code**\n",
    "\n",
    "Here is an example code in Python using the `imbalanced-learn` library to implement SMOTEEN:\n",
    "```python\n",
    "from imblearn.over_sampling import SMOTEEN\n",
    "from sklearn.datasets import make_classification\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "\n",
    "# Generate a classification dataset with class imbalance\n",
    "X, y = make_classification(n_samples=1000, n_features=20, n_informative=10, n_redundant=0, n_repeated=0, n_classes=2, n_clusters_per_class=1, weights=[0.9, 0.1], random_state=42)\n",
    "\n",
    "# Split the dataset into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Create a SMOTEEN object\n",
    "smoteen = SMOTEEN(random_state=42)\n",
    "\n",
    "# Fit the SMOTEEN object to the training data and transform the data\n",
    "X_train_res, y_train_res = smoteen.fit_resample(X_train, y_train)\n",
    "\n",
    "# Train a random forest classifier on the resampled data\n",
    "rf = RandomForestClassifier(n_estimators=100, random_state=42)\n",
    "rf.fit(X_train_res, y_train_res)\n",
    "\n",
    "# Evaluate the model on the test data\n",
    "y_pred = rf.predict(X_test)\n",
    "print(\"Accuracy:\", accuracy_score(y_test, y_pred))\n",
    "print(\"Classification Report:\")\n",
    "print(classification_report(y_test, y_pred))\n",
    "```\n",
    "This code generates a classification dataset with class imbalance, splits the data into training and testing sets, creates a SMOTEEN object, fits the SMOTEEN object to the training data and transforms the data, trains a random forest classifier on the resampled data, and evaluates the model on the test data.\n",
    "\n",
    "**Real-World Example**\n",
    "\n",
    "Let's consider a real-world example of using SMOTEEN to handle class imbalance in a credit card fraud detection dataset. The dataset consists of 100,000 transactions, with 99,000 legitimate transactions and 1,000 fraudulent transactions.\n",
    "\n",
    "| Class | Frequency |\n",
    "| --- | --- |\n",
    "| 0 (Legitimate) | 99,000 |\n",
    "| 1 (Fraudulent) | 1,000 |\n",
    "\n",
    "In this example, the class ratio is 99:1, indicating that the legitimate class has a significantly larger number of instances than the fraudulent class. To handle this class imbalance, we can use SMOTEEN to oversample the minority class (fraudulent transactions) and create synthetic samples that are similar to the existing minority class samples.\n",
    "\n",
    "By using SMOTEEN, we can improve the classification performance of the model and reduce the number of false negatives (fraudulent transactions that are misclassified as legitimate).\n",
    "\n",
    "**Advantages of SMOTEEN**\n",
    "\n",
    "SMOTEEN has several advantages, including:\n",
    "\n",
    "1. **Improved classification performance**: SMOTEEN improves classification performance by combining the predictions of the individual SMOTE models to create a final prediction.\n",
    "2. **Reduced overfitting**: SMOTEEN reduces overfitting by creating an ensemble of multiple SMOTE models, each trained on a different subset of the dataset.\n",
    "3. **Increased robustness**: SMOTEEN increases robustness by creating synthetic samples that are similar to the existing minority class samples.\n",
    "\n",
    "**Disadvantages of SMOTEEN**\n",
    "\n",
    "SMOTEEN has several disadvantages, including:\n",
    "\n",
    "1. **Increased computational complexity**: SMOTEEN increases computational complexity by creating an ensemble of multiple SMOTE models, each trained on a different subset of the dataset.\n",
    "2. **Requires careful tuning**: SMOTEEN requires careful tuning of the hyperparameters to achieve optimal performance.\n",
    "3. **May not work well with high-dimensional data**: SMOTEEN may not work well with high-dimensional data, as the number of features can make it difficult to create synthetic samples that are similar to the existing minority class samples.\n",
    "\n",
    "**Conclusion**\n",
    "\n",
    "SMOTEEN is a variant of the SMOTE algorithm that combines the benefits of SMOTE with ensemble learning. SMOTEEN is designed to handle class imbalance problems in datasets by oversampling the minority class and creating synthetic samples that are similar to the existing minority class samples. By using SMOTEEN, we can improve the classification performance of the model and reduce the number of false negatives. However, SMOTEEN has several disadvantages, including increased computational complexity, requires careful tuning, and may not work well with high-dimensional data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "**Handling High-Dimensional Data with SMOTEEN**\n",
    "\n",
    "When dealing with high-dimensional data, SMOTEEN may not work well due to the curse of dimensionality. In such cases, it's essential to consider alternative strategies to handle class imbalance. Here are some approaches you can take:\n",
    "\n",
    "1. **Feature Selection**: Select a subset of the most relevant features to reduce the dimensionality of the data. This can help improve the performance of SMOTEEN.\n",
    "2. **Dimensionality Reduction**: Use techniques such as PCA (Principal Component Analysis), t-SNE (t-distributed Stochastic Neighbor Embedding), or Autoencoders to reduce the dimensionality of the data.\n",
    "3. **SMOTE Variants**: Consider using SMOTE variants that are specifically designed to handle high-dimensional data, such as:\n",
    "\t* **Borderline-SMOTE**: Focuses on creating synthetic samples near the decision boundary.\n",
    "\t* **Safe-Level-SMOTE**: Creates synthetic samples that are safe from the majority class.\n",
    "\t* **Density-Based SMOTE**: Creates synthetic samples based on the density of the minority class.\n",
    "4. **Ensemble Methods**: Use ensemble methods that combine multiple models trained on different subsets of the data. This can help improve the performance of SMOTEEN.\n",
    "5. **Data Preprocessing**: Apply data preprocessing techniques such as normalization, feature scaling, or encoding categorical variables to improve the quality of the data.\n",
    "6. **Hybrid Approach**: Combine SMOTEEN with other class imbalance techniques, such as oversampling, undersampling, or cost-sensitive learning.\n",
    "7. **Deep Learning**: Consider using deep learning models that are designed to handle high-dimensional data, such as Convolutional Neural Networks (CNNs) or Recurrent Neural Networks (RNNs).\n",
    "\n",
    "**Example Code**\n",
    "\n",
    "Here's an example code in Python using the `imbalanced-learn` library to implement SMOTEEN with feature selection:\n",
    "```python\n",
    "from imblearn.over_sampling import SMOTEEN\n",
    "from sklearn.datasets import make_classification\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "from sklearn.feature_selection import SelectKBest, f_classif\n",
    "\n",
    "# Generate a classification dataset with class imbalance\n",
    "X, y = make_classification(n_samples=1000, n_features=20, n_informative=10, n_redundant=0, n_repeated=0, n_classes=2, n_clusters_per_class=1, weights=[0.9, 0.1], random_state=42)\n",
    "\n",
    "# Split the dataset into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Select the top 10 features using f-classif\n",
    "selector = SelectKBest(f_classif, k=10)\n",
    "X_train_selected = selector.fit_transform(X_train, y_train)\n",
    "X_test_selected = selector.transform(X_test)\n",
    "\n",
    "# Create a SMOTEEN object\n",
    "smoteen = SMOTEEN(random_state=42)\n",
    "\n",
    "# Fit the SMOTEEN object to the training data and transform the data\n",
    "X_train_res, y_train_res = smoteen.fit_resample(X_train_selected, y_train)\n",
    "\n",
    "# Train a random forest classifier on the resampled data\n",
    "rf = RandomForestClassifier(n_estimators=100, random_state=42)\n",
    "rf.fit(X_train_res, y_train_res)\n",
    "\n",
    "# Evaluate the model on the test data\n",
    "y_pred = rf.predict(X_test_selected)\n",
    "print(\"Accuracy:\", accuracy_score(y_test, y_pred))\n",
    "print(\"Classification Report:\")\n",
    "print(classification_report(y_test, y_pred))\n",
    "```\n",
    "This code selects the top 10 features using f-classif and then applies SMOTEEN to the selected features. The resulting model is trained on the resampled data and evaluated on the test data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
